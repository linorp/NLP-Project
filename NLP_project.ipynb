{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOCsIpvvdGJvievn3Sem/qu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/linorp/NLP-Project/blob/main/NLP_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSgiZfCHs5Fr"
      },
      "source": [
        "### **Create Full Data - No slicing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bi3-8TNnkpeo"
      },
      "source": [
        "!pip install json_lines"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuFKtZi6s39_"
      },
      "source": [
        "import json_lines\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "def create_clusters_format(clusters):\n",
        "  clusters_format = []\n",
        "  for i in range(len(clusters)):\n",
        "    for j in range(len(clusters[i])):\n",
        "      clusters[i][j].append(True)\n",
        "  return clusters\n",
        "\n",
        "def create_clusters_format_overlapping(clusters_format):\n",
        "  overlapping_indexes = set()\n",
        "  for i in range(len(clusters_format)):\n",
        "    for j in range(len(clusters_format[i])):\n",
        "      for t in range(clusters_format[i][j][0], clusters_format[i][j][1]+1):\n",
        "        if t in overlapping_indexes:\n",
        "          clusters_format[i][j][2] = False\n",
        "          break\n",
        "        else:\n",
        "          overlapping_indexes.add(t)\n",
        "  return clusters_format\n",
        "\n",
        "def map_word_to_cluster(clusters_format):\n",
        "  d = {}\n",
        "  for i in range(len(clusters_format)):\n",
        "    for j in range(len(clusters_format[i])):\n",
        "      if clusters_format[i][j][2] == False:\n",
        "        continue\n",
        "      for index in range(clusters_format[i][j][0], clusters_format[i][j][1]+1):\n",
        "        assert index not in d\n",
        "        d[index]=i\n",
        "  return d\n",
        "\n",
        "def parse_json_line(line):\n",
        "  #data = json.loads(line.strip())\n",
        "  clusters = line['clusters']\n",
        "  clusters_format = create_clusters_format(clusters)\n",
        "  clusters_format = create_clusters_format_overlapping(clusters_format)\n",
        "  map_words = map_word_to_cluster(clusters_format)\n",
        "  # print(clusters_format)\n",
        "  # print(map_words)\n",
        "  word_index = 0\n",
        "  input= \"\"\n",
        "  output = \"\"\n",
        "\n",
        "  sentences = line['sentences']\n",
        "  # print(sentences)\n",
        "  for i in range(len(sentences)):\n",
        "    for j in range(len(sentences[i])):\n",
        "      if word_index in map_words:\n",
        "        cluster = str(map_words.get(word_index))\n",
        "      else:\n",
        "        cluster = \"-\"\n",
        "      input += sentences[i][j] + \" \"\n",
        "      output += cluster + \" \"\n",
        "      word_index += 1\n",
        "  return input, output\n",
        "\n",
        "def parse_input_file(pathInput,pathOutput,type):\n",
        "  output_file = []\n",
        "  with open (pathInput, 'r') as file:\n",
        "    count = 0\n",
        "    for line in json_lines.reader(file):\n",
        "      input, output = parse_json_line(line)\n",
        "      output_file.append([input, output])\n",
        "      count += 1\n",
        "      # if count == 1000:\n",
        "      #   break\n",
        "  print(count)\n",
        "  output_file_panda = pd.DataFrame(output_file, columns=['input', 'output'])\n",
        "  if type == 'T5':\n",
        "    output_file_panda['input'] = \"coref: \" + output_file_panda['input']\n",
        "  output_file_panda.to_csv(pathOutput,index=False)\n",
        "\n",
        "def main():\n",
        "  output_path_train_T5 = '/content/output/train_T5_format.csv'\n",
        "  output_path_test_T5 = '/content/output/test_T5_format.csv'\n",
        "  output_path_dev_T5 = '/content/output/dev_T5_format.csv'\n",
        "\n",
        "  output_path_train_Bart = '/content/output/train_Bart_format.csv'\n",
        "  output_path_test_Bart = '/content/output/test_Bart_format.csv'\n",
        "  output_path_dev_Bart = '/content/output/dev_Bart_format.csv'\n",
        "\n",
        "  input_path_train = '/content/input/train.jsonlines'\n",
        "  input_path_test = '/content/input/test.jsonlines'\n",
        "  input_path_dev = '/content/input/dev.jsonlines'\n",
        "\n",
        "  type = 'Bart'\n",
        "  #type = 'T5'\n",
        "  if type == 'T5':\n",
        "    parse_input_file(input_path_train,output_path_train_T5,type)\n",
        "    parse_input_file(input_path_test,output_path_test_T5,type)\n",
        "    parse_input_file(input_path_dev,output_path_dev_T5,type)\n",
        "  else:\n",
        "    parse_input_file(input_path_train,output_path_train_Bart,type)\n",
        "    parse_input_file(input_path_test,output_path_test_Bart,type)\n",
        "    parse_input_file(input_path_dev,output_path_dev_Bart,type)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAwcs7Ep7qvz"
      },
      "source": [
        "### **Create Data With Slicing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRR_gYP8-yYJ"
      },
      "source": [
        "!pip install json_lines"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2E5JOGOa7In1"
      },
      "source": [
        "import json_lines\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "def create_clusters_format(clusters, a):\n",
        "  clusters_format = []\n",
        "  for i in range(len(clusters)):\n",
        "    for j in range(len(clusters[i])):\n",
        "      if a == 0:\n",
        "        clusters[i][j].append(True)\n",
        "  return clusters\n",
        "\n",
        "def create_clusters_format_overlapping(clusters_format):\n",
        "  overlapping_indexes = set()\n",
        "  for i in range(len(clusters_format)):\n",
        "    for j in range(len(clusters_format[i])):\n",
        "      for t in range(clusters_format[i][j][0], clusters_format[i][j][1]+1):\n",
        "        if t in overlapping_indexes:\n",
        "          clusters_format[i][j][2] = False\n",
        "          break\n",
        "        else:\n",
        "          overlapping_indexes.add(t)\n",
        "  return clusters_format\n",
        "\n",
        "def map_word_to_cluster(clusters_format, start_index, end_index):\n",
        "  d = {}\n",
        "  clu_index = {}\n",
        "  count = 0\n",
        "  count_words = 0\n",
        "  for i in range(len(clusters_format)):\n",
        "    for j in range(len(clusters_format[i])):\n",
        "      if clusters_format[i][j][2] == False:\n",
        "        continue\n",
        "      if clusters_format[i][j][0] >= start_index & clusters_format[i][j][0] <= end_index:\n",
        "        count_words +=1\n",
        "        if count_words >= 2:\n",
        "          #assert j not in clu_index:\n",
        "          clu_index[i] = count\n",
        "          count += 1\n",
        "          count_words = 0\n",
        "          break\n",
        "\n",
        "  for i in range(len(clusters_format)):\n",
        "    for j in range(len(clusters_format[i])):\n",
        "      if i in clu_index:\n",
        "        for index in range(clusters_format[i][j][0], clusters_format[i][j][1]+1):\n",
        "          #assert index not in d\n",
        "          d[index]=clu_index[i]\n",
        "  return d\n",
        "\n",
        "def parse_json_line(line, start, end):\n",
        "  #data = json.loads(line.strip())\n",
        "  word_index = 0\n",
        "  word_end = 0\n",
        "  sentences = line['sentences']\n",
        "  for i in range(start):\n",
        "    for j in range(len(sentences[i])):\n",
        "      word_index += 1\n",
        "  \n",
        "  for i in range(end):\n",
        "    for j in range(len(sentences[i])):\n",
        "      word_end += 1\n",
        "\n",
        "  clusters = line['clusters']\n",
        "  \n",
        "  clusters_format = create_clusters_format(clusters, start)\n",
        "  if (start == 0):\n",
        "    clusters_format = create_clusters_format_overlapping(clusters_format)\n",
        "  map_words = map_word_to_cluster(clusters_format, word_index, word_end)\n",
        "\n",
        "  input= \"\"\n",
        "  output = \"\"\n",
        "\n",
        "  count = 0\n",
        "  count_d = {}\n",
        "  for i in range(start, end):\n",
        "    for j in range(len(sentences[i])):\n",
        "      if word_index in map_words:\n",
        "        if not map_words.get(word_index) in count_d:\n",
        "          count_d[map_words.get(word_index)] = count\n",
        "          count += 1\n",
        "        cluster = str(count_d[map_words.get(word_index)])\n",
        "      else:\n",
        "        cluster = \"-\"\n",
        "      input += sentences[i][j] + \" \"\n",
        "      output += cluster + \" \"\n",
        "      word_index += 1\n",
        "  return input, output\n",
        "\n",
        "def parse_input_file(pathInput,pathOutput,type):\n",
        "  output_file = []\n",
        "  count = 0\n",
        "  with open (pathInput, 'r') as file:\n",
        "    for line in json_lines.reader(file):\n",
        "      flag = 0\n",
        "      a = 0\n",
        "      #print(len(line[\"sentences\"]))\n",
        "      while a < (len(line[\"sentences\"])):\n",
        "        if (a+5 < len(line[\"sentences\"])):\n",
        "          input, output = parse_json_line(line, a, a+5)\n",
        "        else:\n",
        "          input, output = parse_json_line(line, a, len(line[\"sentences\"]))\n",
        "        #print(input)\n",
        "        #print(output)\n",
        "        output_file.append([input, output])\n",
        "        a += 5\n",
        "        count += 1\n",
        "        if count == 50:\n",
        "          flag = 1\n",
        "          break \n",
        "      if flag == 1:\n",
        "        break\n",
        "  \n",
        "  output_file_panda = pd.DataFrame(output_file, columns=['input', 'output'])\n",
        "  if type == 'T5':\n",
        "    output_file_panda['input'] = \"coref: \" + output_file_panda['input']\n",
        "  output_file_panda.to_csv(pathOutput,index=False)\n",
        "\n",
        "def main():\n",
        "  output_path_train_T5 = '/content/output/train_T5_format.csv'\n",
        "  output_path_test_T5 = '/content/output/test_T5_format.csv'\n",
        "  output_path_dev_T5 = '/content/output/dev_T5_format.csv'\n",
        "\n",
        "  output_path_train_Bart = '/content/output/train_Bart_format.csv'\n",
        "  output_path_test_Bart = '/content/output/test_Bart_format.csv'\n",
        "  output_path_dev_Bart = '/content/output/dev_Bart_format.csv'\n",
        "\n",
        "  input_path_train = '/content/input/train.jsonlines'\n",
        "  input_path_test = '/content/input/test.jsonlines'\n",
        "  input_path_dev = '/content/input/dev.jsonlines'\n",
        "\n",
        "  #type = 'Bart'\n",
        "  type = 'T5'\n",
        "  if type == 'T5':\n",
        "    parse_input_file(input_path_train,output_path_train_T5,type)\n",
        "    parse_input_file(input_path_test,output_path_test_T5,type)\n",
        "    parse_input_file(input_path_dev,output_path_dev_T5,type)\n",
        "  else:\n",
        "    parse_input_file(input_path_train,output_path_train_Bart,type)\n",
        "    parse_input_file(input_path_test,output_path_test_Bart,type)\n",
        "    parse_input_file(input_path_dev,output_path_dev_Bart,type)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZ9SHyvYiYNp"
      },
      "source": [
        "### **Create Data - With Augmentation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbxLOmLyksCB"
      },
      "source": [
        "!pip install json_lines"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AgjLd2ZicTa"
      },
      "source": [
        "import json_lines\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "def create_clusters_format(clusters, a):\n",
        "  clusters_format = []\n",
        "  for i in range(len(clusters)):\n",
        "    for j in range(len(clusters[i])):\n",
        "      if a == 0:\n",
        "        clusters[i][j].append(True)\n",
        "  return clusters\n",
        "\n",
        "def create_clusters_format_overlapping(clusters_format):\n",
        "  overlapping_indexes = set()\n",
        "  for i in range(len(clusters_format)):\n",
        "    for j in range(len(clusters_format[i])):\n",
        "      for t in range(clusters_format[i][j][0], clusters_format[i][j][1]+1):\n",
        "        if t in overlapping_indexes:\n",
        "          clusters_format[i][j][2] = False\n",
        "          break\n",
        "        else:\n",
        "          overlapping_indexes.add(t)\n",
        "  return clusters_format\n",
        "\n",
        "def map_word_to_cluster(clusters_format, start_index):\n",
        "  d = {}\n",
        "  clu_index = {}\n",
        "  count = 0\n",
        "  count_words = 0\n",
        "  for i in range(len(clusters_format)):\n",
        "    for j in range(len(clusters_format[i])):\n",
        "      if clusters_format[i][j][2] == False:\n",
        "        continue\n",
        "      if clusters_format[i][j][0] >= start_index:\n",
        "        count_words +=1\n",
        "        if count_words >= 2:\n",
        "          #assert j not in clu_index:\n",
        "          clu_index[i] = count\n",
        "          count += 1\n",
        "          count_words = 0\n",
        "          break\n",
        "\n",
        "  for i in range(len(clusters_format)):\n",
        "    for j in range(len(clusters_format[i])):\n",
        "      if i in clu_index:\n",
        "        for index in range(clusters_format[i][j][0], clusters_format[i][j][1]+1):\n",
        "          #assert index not in d\n",
        "          d[index]=clu_index[i]\n",
        "  return d\n",
        "\n",
        "def parse_json_line(line, a):\n",
        "  #data = json.loads(line.strip())\n",
        "  word_index = 0\n",
        "  sentences = line['sentences']\n",
        "  for i in range(a):\n",
        "    for j in range(len(sentences[i])):\n",
        "      word_index += 1\n",
        "\n",
        "  clusters = line['clusters']\n",
        "  \n",
        "  clusters_format = create_clusters_format(clusters, a)\n",
        "  if (a == 0):\n",
        "    clusters_format = create_clusters_format_overlapping(clusters_format)\n",
        "  map_words = map_word_to_cluster(clusters_format, word_index)\n",
        "\n",
        "  input= \"\"\n",
        "  output = \"\"\n",
        "\n",
        "  count = 0\n",
        "  count_d = {}\n",
        "  for i in range(a, len(sentences)):\n",
        "    for j in range(len(sentences[i])):\n",
        "      if word_index in map_words:\n",
        "        if not map_words.get(word_index) in count_d:\n",
        "          count_d[map_words.get(word_index)] = count\n",
        "          count += 1\n",
        "        cluster = str(count_d[map_words.get(word_index)])\n",
        "      else:\n",
        "        cluster = \"-\"\n",
        "      input += sentences[i][j] + \" \"\n",
        "      output += cluster + \" \"\n",
        "      word_index += 1\n",
        "  return input, output\n",
        "\n",
        "def parse_input_file(pathInput,pathOutput,type):\n",
        "  output_file = []\n",
        "  count = 0\n",
        "  with open (pathInput, 'r') as file:\n",
        "    for line in json_lines.reader(file):\n",
        "      for a in range(len(line[\"sentences\"])):\n",
        "        input, output = parse_json_line(line, a)\n",
        "        output_file.append([input, output])\n",
        "        count +=1\n",
        "  print(count)\n",
        "  \n",
        "  output_file_panda = pd.DataFrame(output_file, columns=['input', 'output'])\n",
        "  if type == 'T5':\n",
        "    output_file_panda['input'] = \"coref: \" + output_file_panda['input']\n",
        "  output_file_panda.to_csv(pathOutput,index=False)\n",
        "\n",
        "def main():\n",
        "  output_path_train_T5 = '/content/output/train_T5_format.csv'\n",
        "  output_path_test_T5 = '/content/output/test_T5_format.csv'\n",
        "  output_path_dev_T5 = '/content/output/dev_T5_format.csv'\n",
        "\n",
        "  output_path_train_Bart = '/content/output/train_Bart_format.csv'\n",
        "  output_path_test_Bart = '/content/output/test_Bart_format.csv'\n",
        "  output_path_dev_Bart = '/content/output/dev_Bart_format.csv'\n",
        "\n",
        "  input_path_train = '/content/input/train.jsonlines'\n",
        "  input_path_test = '/content/input/test.jsonlines'\n",
        "  input_path_dev = '/content/input/dev.jsonlines'\n",
        "\n",
        "  type = 'Bart'\n",
        "  #type = 'T5'\n",
        "  if type == 'T5':\n",
        "    parse_input_file(input_path_train,output_path_train_T5,type)\n",
        "    parse_input_file(input_path_test,output_path_test_T5,type)\n",
        "    parse_input_file(input_path_dev,output_path_dev_T5,type)\n",
        "  else:\n",
        "    parse_input_file(input_path_train,output_path_train_Bart,type)\n",
        "    parse_input_file(input_path_test,output_path_test_Bart,type)\n",
        "    parse_input_file(input_path_dev,output_path_dev_Bart,type)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00U0jkd273vv"
      },
      "source": [
        "### **T5 - model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvrWmCK9pqRD"
      },
      "source": [
        "# install libraries\n",
        "!pip install sentencepiece\n",
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install rich[jupyter]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbww__Lt77Nl"
      },
      "source": [
        "# Importing libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import os\n",
        "\n",
        "# Importing the T5 modules from huggingface/transformers\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# rich: for a better display on terminal\n",
        "from rich.table import Column, Table\n",
        "from rich import box\n",
        "from rich.console import Console\n",
        "\n",
        "# define a rich console logger\n",
        "console = Console(record=True)\n",
        "\n",
        "# to display dataframe in ASCII format\n",
        "def display_df(df):\n",
        "    \"\"\"display dataframe in ASCII format\"\"\"\n",
        "\n",
        "    console = Console()\n",
        "    table = Table(\n",
        "        Column(\"source_text\", justify=\"center\"),\n",
        "        Column(\"target_text\", justify=\"center\"),\n",
        "        title=\"Sample Data\",\n",
        "        pad_edge=False,\n",
        "        box=box.ASCII,\n",
        "    )\n",
        "\n",
        "    for i, row in enumerate(df.values.tolist()):\n",
        "        table.add_row(row[0], row[1])\n",
        "\n",
        "    console.print(table)\n",
        "\n",
        "# training logger to log training progress\n",
        "training_logger = Table(\n",
        "    Column(\"Epoch\", justify=\"center\"),\n",
        "    Column(\"Steps\", justify=\"center\"),\n",
        "    Column(\"Loss\", justify=\"center\"),\n",
        "    title=\"Training Status\",\n",
        "    pad_edge=False,\n",
        "    box=box.ASCII,\n",
        ")\n",
        "\n",
        "# Setting up the device for GPU usage\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "\n",
        "class YourDataSetClass(Dataset):\n",
        "    \"\"\"\n",
        "    Creating a custom dataset for reading the dataset and\n",
        "    loading it into the dataloader to pass it to the\n",
        "    neural network for finetuning the model\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, dataframe, tokenizer, source_len, target_len, source_text, target_text\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initializes a Dataset class\n",
        "\n",
        "        Args:\n",
        "            dataframe (pandas.DataFrame): Input dataframe\n",
        "            tokenizer (transformers.tokenizer): Transformers tokenizer\n",
        "            source_len (int): Max length of source text\n",
        "            target_len (int): Max length of target text\n",
        "            source_text (str): column name of source text\n",
        "            target_text (str): column name of target text\n",
        "        \"\"\"\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.source_len = source_len\n",
        "        self.summ_len = target_len\n",
        "        self.target_text = self.data[target_text]\n",
        "        self.source_text = self.data[source_text]\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"returns the length of dataframe\"\"\"\n",
        "\n",
        "        return len(self.target_text)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"return the input ids, attention masks and target ids\"\"\"\n",
        "\n",
        "        source_text = str(self.source_text[index])\n",
        "        target_text = str(self.target_text[index])\n",
        "\n",
        "        # cleaning data so as to ensure data is in string type\n",
        "        source_text = \" \".join(source_text.split())\n",
        "        target_text = \" \".join(target_text.split())\n",
        "\n",
        "        source = self.tokenizer.batch_encode_plus(\n",
        "            [source_text],\n",
        "            max_length=self.source_len,\n",
        "            pad_to_max_length=True,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        target = self.tokenizer.batch_encode_plus(\n",
        "            [target_text],\n",
        "            max_length=self.summ_len,\n",
        "            pad_to_max_length=True,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        source_ids = source[\"input_ids\"].squeeze()\n",
        "        source_mask = source[\"attention_mask\"].squeeze()\n",
        "        target_ids = target[\"input_ids\"].squeeze()\n",
        "        target_mask = target[\"attention_mask\"].squeeze()\n",
        "\n",
        "        return {\n",
        "            \"source_ids\": source_ids.to(dtype=torch.long),\n",
        "            \"source_mask\": source_mask.to(dtype=torch.long),\n",
        "            \"target_ids\": target_ids.to(dtype=torch.long),\n",
        "            \"target_ids_y\": target_ids.to(dtype=torch.long),\n",
        "        }\n",
        "\n",
        "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
        "\n",
        "    \"\"\"\n",
        "    Function to be called for training with the parameters passed from main function\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    model.train()\n",
        "    train_losses = 0.0\n",
        "    for _, data in enumerate(loader, 0):\n",
        "        y = data[\"target_ids\"].to(device, dtype=torch.long)\n",
        "        y_ids = y[:, :-1].contiguous()\n",
        "        lm_labels = y[:, 1:].clone().detach()\n",
        "        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
        "        ids = data[\"source_ids\"].to(device, dtype=torch.long)\n",
        "        mask = data[\"source_mask\"].to(device, dtype=torch.long)\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=ids,\n",
        "            attention_mask=mask,\n",
        "            decoder_input_ids=y_ids,\n",
        "            labels=lm_labels,\n",
        "        )\n",
        "        loss = outputs[0]\n",
        "        train_losses += loss\n",
        "\n",
        "        if _ % 5000 == 0:\n",
        "            training_logger.add_row(str(epoch), str(_), str(loss))\n",
        "            console.print(training_logger)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    average_train_loss = train_losses / len(loader)\n",
        "    return average_train_loss\n",
        "\n",
        "def validate_on_dev(epoch, tokenizer, model, device, loader, optimizer):\n",
        "\n",
        "    \"\"\"\n",
        "    Function to be called for training with the parameters passed from main function\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "    dev_losses = 0.0\n",
        "    with torch.no_grad():\n",
        "        for _, data in enumerate(loader, 0):\n",
        "            y = data[\"target_ids\"].to(device, dtype=torch.long)\n",
        "            y_ids = y[:, :-1].contiguous()\n",
        "            lm_labels = y[:, 1:].clone().detach()\n",
        "            lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
        "            ids = data[\"source_ids\"].to(device, dtype=torch.long)\n",
        "            mask = data[\"source_mask\"].to(device, dtype=torch.long)\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=ids,\n",
        "                attention_mask=mask,\n",
        "                decoder_input_ids=y_ids,\n",
        "                labels=lm_labels,\n",
        "            )\n",
        "            loss = outputs[0]\n",
        "            dev_losses += loss\n",
        "\n",
        "            if _ % 10000 == 0:\n",
        "                training_logger.add_row(str(epoch), str(_), str(loss))\n",
        "                console.print(training_logger)\n",
        "\n",
        "    average_dev_loss = dev_losses / len(loader)\n",
        "    return average_dev_loss\n",
        "\n",
        "\n",
        "def validate(epoch, tokenizer, model, device, loader):\n",
        "\n",
        "  \"\"\"\n",
        "  Function to evaluate model for predictions\n",
        "\n",
        "  \"\"\"\n",
        "  model.eval()\n",
        "  predictions = []\n",
        "  actuals = []\n",
        "  with torch.no_grad():\n",
        "      for _, data in enumerate(loader, 0):\n",
        "          y = data['target_ids'].to(device, dtype = torch.long)\n",
        "          ids = data['source_ids'].to(device, dtype = torch.long)\n",
        "          mask = data['source_mask'].to(device, dtype = torch.long)\n",
        "\n",
        "          generated_ids = model.generate(\n",
        "              input_ids = ids,\n",
        "              attention_mask = mask, \n",
        "              max_length=150, \n",
        "              num_beams=2,\n",
        "              repetition_penalty=2.5, \n",
        "              length_penalty=1.0, \n",
        "              early_stopping=True\n",
        "              )\n",
        "          preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
        "          target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
        "          if _%1000==0:\n",
        "              console.print(f'Completed {_}')\n",
        "\n",
        "          predictions.extend(preds)\n",
        "          actuals.extend(target)\n",
        "  return predictions, actuals   \n",
        "\n",
        "def T5Trainer(\n",
        "    dataframe, source_text, target_text, model_params, output_dir=\"./outputs/\"\n",
        "):\n",
        "\n",
        "    \"\"\"\n",
        "    T5 trainer\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Set random seeds and deterministic pytorch for reproducibility\n",
        "    torch.manual_seed(model_params[\"SEED\"])  # pytorch random seed\n",
        "    np.random.seed(model_params[\"SEED\"])  # numpy random seed\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "    # logging\n",
        "    console.log(f\"\"\"[Model]: Loading {model_params[\"MODEL\"]}...\\n\"\"\")\n",
        "\n",
        "    # tokenzier for encoding the text\n",
        "    tokenizer = T5Tokenizer.from_pretrained(model_params[\"MODEL\"],dropout=0.3)\n",
        "\n",
        "    # Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary.\n",
        "    # Further this model is sent to device (GPU/TPU) for using the hardware.\n",
        "    model = T5ForConditionalGeneration.from_pretrained(model_params[\"MODEL\"])\n",
        "    model = model.to(device)\n",
        "\n",
        "    # logging\n",
        "    console.log(f\"[Data]: Reading data...\\n\")\n",
        "\n",
        "    # Importing the raw dataset\n",
        "    dataframe = dataframe[[source_text, target_text]]\n",
        "    display_df(dataframe.head(2))\n",
        "\n",
        "    # Creation of Dataset and Dataloader\n",
        "    # Defining the train size. So 80% of the data will be used for training and the rest for validation.\n",
        "    #train_size = 0.8\n",
        "    #train_dataset = dataframe.sample(frac=train_size, random_state=model_params[\"SEED\"])\n",
        "    #val_dataset = dataframe.drop(train_dataset.index).reset_index(drop=True)\n",
        "    #train_dataset = train_dataset.reset_index(drop=True)\n",
        "    train_dataset = dataframe\n",
        "\n",
        "    console.print(f\"FULL Dataset: {dataframe.shape}\")\n",
        "    console.print(f\"TRAIN Dataset: {train_dataset.shape}\")\n",
        "    console.print(f\"DEV Dataset: {dev_dataset.shape}\")\n",
        "    console.print(f\"TEST Dataset: {val_dataset.shape}\\n\")\n",
        "\n",
        "    # Creating the Training and Validation dataset for further creation of Dataloader\n",
        "    training_set = YourDataSetClass(\n",
        "        train_dataset,\n",
        "        tokenizer,\n",
        "        model_params[\"MAX_SOURCE_TEXT_LENGTH\"],\n",
        "        model_params[\"MAX_TARGET_TEXT_LENGTH\"],\n",
        "        source_text,\n",
        "        target_text,\n",
        "    )\n",
        "    val_set = YourDataSetClass(\n",
        "        val_dataset,\n",
        "        tokenizer,\n",
        "        model_params[\"MAX_SOURCE_TEXT_LENGTH\"],\n",
        "        model_params[\"MAX_TARGET_TEXT_LENGTH\"],\n",
        "        source_text,\n",
        "        target_text,\n",
        "    )\n",
        "\n",
        "    dev_set = YourDataSetClass(\n",
        "        dev_dataset,\n",
        "        tokenizer,\n",
        "        model_params[\"MAX_SOURCE_TEXT_LENGTH\"],\n",
        "        model_params[\"MAX_TARGET_TEXT_LENGTH\"],\n",
        "        source_text,\n",
        "        target_text,\n",
        "    )\n",
        "\n",
        "    # Defining the parameters for creation of dataloaders\n",
        "    train_params = {\n",
        "        \"batch_size\": model_params[\"TRAIN_BATCH_SIZE\"],\n",
        "        \"shuffle\": True,\n",
        "        \"num_workers\": 0,\n",
        "    }\n",
        "\n",
        "    val_params = {\n",
        "        \"batch_size\": model_params[\"VALID_BATCH_SIZE\"],\n",
        "        \"shuffle\": False,\n",
        "        \"num_workers\": 0,\n",
        "    }\n",
        "    dev_params = {\n",
        "        \"batch_size\": model_params[\"VALID_BATCH_SIZE\"],\n",
        "        \"shuffle\": False,\n",
        "        \"num_workers\": 0,\n",
        "    }\n",
        "\n",
        "    # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n",
        "    training_loader = DataLoader(training_set, **train_params)\n",
        "    val_loader = DataLoader(val_set, **val_params)\n",
        "    dev_loader = DataLoader(dev_set, **dev_params)\n",
        "\n",
        "    # Defining the optimizer that will be used to tune the weights of the network in the training session.\n",
        "    optimizer = torch.optim.Adam(\n",
        "        params=model.parameters(), lr=model_params[\"LEARNING_RATE\"]\n",
        "    )\n",
        "\n",
        "    # Training loop\n",
        "    console.log(f\"[Initiating Fine Tuning]...\\n\")\n",
        "\n",
        "    for epoch in range(model_params[\"TRAIN_EPOCHS\"]):\n",
        "        train_loss = train(epoch, tokenizer, model, device, training_loader, optimizer)\n",
        "        # TODO: add dev loss evaluate + report\n",
        "        dev_loss = validate_on_dev(epoch, tokenizer, model, device, dev_loader, optimizer)\n",
        "        console.log(f\"epoch {epoch}, training loss: {train_loss}, dev loss: {dev_loss}\")\n",
        "\n",
        "    console.log(f\"[Saving Model]...\\n\")\n",
        "    # Saving the model after training\n",
        "    path = os.path.join(output_dir, \"model_files\")\n",
        "    model.save_pretrained(path)\n",
        "    tokenizer.save_pretrained(path)\n",
        "\n",
        "    # evaluating test dataset\n",
        "    console.log(f\"[Initiating Validation]...\\n\")\n",
        "    for epoch in range(model_params[\"VAL_EPOCHS\"]):\n",
        "        predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n",
        "        final_df = pd.DataFrame({\"Generated Text\": predictions, \"Actual Text\": actuals})\n",
        "        final_df.to_csv(os.path.join(output_dir, \"predictions.csv\"))\n",
        "\n",
        "    console.save_text(os.path.join(output_dir, \"logs.txt\"))\n",
        "\n",
        "    console.log(f\"[Validation Completed.]\\n\")\n",
        "    console.print(\n",
        "        f\"\"\"[Model] Model saved @ {os.path.join(output_dir, \"model_files\")}\\n\"\"\"\n",
        "    )\n",
        "    console.print(\n",
        "        f\"\"\"[Validation] Generation on Validation data saved @ {os.path.join(output_dir,'predictions.csv')}\\n\"\"\"\n",
        "    )\n",
        "    console.print(f\"\"\"[Logs] Logs saved @ {os.path.join(output_dir,'logs.txt')}\\n\"\"\")       \n",
        "\n",
        "\n",
        "# 1) Change model to t5-base\n",
        "# 2) Train epochs: 7/8\n",
        "# 3) Split the data (documents) every X sentences\n",
        "# Goal: generated.csv, outputs/predictions.csv\n",
        "\n",
        "model_params = {\n",
        "    \"MODEL\": \"google/t5-v1_1-base\",  # model_type: t5-base/t5-large\n",
        "    \"TRAIN_BATCH_SIZE\": 1,  # training batch size\n",
        "    \"VALID_BATCH_SIZE\": 1,  # validation batch size\n",
        "    \"TRAIN_EPOCHS\": 3,  # number of training epochs\n",
        "    \"VAL_EPOCHS\": 1,  # number of validation epochs\n",
        "    \"LEARNING_RATE\": 1e-4,  # learning rate\n",
        "    \"MAX_SOURCE_TEXT_LENGTH\": 512,  # max length of source text\n",
        "    \"MAX_TARGET_TEXT_LENGTH\": 512,  # max length of target text\n",
        "    \"SEED\": 42,  # set seed for reproducibility\n",
        "}\n",
        "\n",
        "train_path = '/content/output/train_T5_format.csv'\n",
        "dev_path = '/content/output/dev_T5_format.csv'\n",
        "test_path = '/content/output/test_T5_format.csv'\n",
        "\n",
        "df = pd.read_csv(train_path)\n",
        "dev_dataset = pd.read_csv(dev_path)\n",
        "val_dataset = pd.read_csv(test_path)\n",
        "# TODO: check that data is read correctly (with commas in the input)\n",
        "\n",
        "# T5 accepts prefix of the task to be performed:\n",
        "\n",
        "T5Trainer(\n",
        "    dataframe=df,\n",
        "    source_text=\"input\",\n",
        "    target_text=\"output\",\n",
        "    model_params=model_params,\n",
        "    output_dir=\"/content/outputsT5/\",\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seisvxwBbafl"
      },
      "source": [
        "### **Bart - model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UA5qQThbp-N"
      },
      "source": [
        "# install libraries\n",
        "!pip install sentencepiece\n",
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install rich[jupyter]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnWUPmdqbhjs"
      },
      "source": [
        "# Importing libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import os\n",
        "\n",
        "# Importing the BART modules from huggingface/transformers\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "\n",
        "\n",
        "# rich: for a better display on terminal\n",
        "from rich.table import Column, Table\n",
        "from rich import box\n",
        "from rich.console import Console\n",
        "\n",
        "# define a rich console logger\n",
        "console = Console(record=True)\n",
        "\n",
        "# to display dataframe in ASCII format\n",
        "def display_df(df):\n",
        "    \"\"\"display dataframe in ASCII format\"\"\"\n",
        "\n",
        "    console = Console()\n",
        "    table = Table(\n",
        "        Column(\"source_text\", justify=\"center\"),\n",
        "        Column(\"target_text\", justify=\"center\"),\n",
        "        title=\"Sample Data\",\n",
        "        pad_edge=False,\n",
        "        box=box.ASCII,\n",
        "    )\n",
        "\n",
        "    for i, row in enumerate(df.values.tolist()):\n",
        "        table.add_row(row[0], row[1])\n",
        "\n",
        "    console.print(table)\n",
        "\n",
        "# training logger to log training progress\n",
        "training_logger = Table(\n",
        "    Column(\"Epoch\", justify=\"center\"),\n",
        "    Column(\"Steps\", justify=\"center\"),\n",
        "    Column(\"Loss\", justify=\"center\"),\n",
        "    title=\"Training Status\",\n",
        "    pad_edge=False,\n",
        "    box=box.ASCII,\n",
        ")\n",
        "\n",
        "# Setting up the device for GPU usage\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "\n",
        "class YourDataSetClass(Dataset):\n",
        "    \"\"\"\n",
        "    Creating a custom dataset for reading the dataset and\n",
        "    loading it into the dataloader to pass it to the\n",
        "    neural network for finetuning the model\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, dataframe, tokenizer, source_len, target_len, source_text, target_text\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initializes a Dataset class\n",
        "\n",
        "        Args:\n",
        "            dataframe (pandas.DataFrame): Input dataframe\n",
        "            tokenizer (transformers.tokenizer): Transformers tokenizer\n",
        "            source_len (int): Max length of source text\n",
        "            target_len (int): Max length of target text\n",
        "            source_text (str): column name of source text\n",
        "            target_text (str): column name of target text\n",
        "        \"\"\"\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.source_len = source_len\n",
        "        self.summ_len = target_len\n",
        "        self.target_text = self.data[target_text]\n",
        "        self.source_text = self.data[source_text]\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"returns the length of dataframe\"\"\"\n",
        "\n",
        "        return len(self.target_text)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"return the input ids, attention masks and target ids\"\"\"\n",
        "\n",
        "        source_text = str(self.source_text[index])\n",
        "        target_text = str(self.target_text[index])\n",
        "\n",
        "        # cleaning data so as to ensure data is in string type\n",
        "        source_text = \" \".join(source_text.split())\n",
        "        target_text = \" \".join(target_text.split())\n",
        "\n",
        "        source = self.tokenizer.batch_encode_plus(\n",
        "            [source_text],\n",
        "            max_length=self.source_len,\n",
        "            pad_to_max_length=True,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        target = self.tokenizer.batch_encode_plus(\n",
        "            [target_text],\n",
        "            max_length=self.summ_len,\n",
        "            pad_to_max_length=True,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        source_ids = source[\"input_ids\"].squeeze()\n",
        "        source_mask = source[\"attention_mask\"].squeeze()\n",
        "        target_ids = target[\"input_ids\"].squeeze()\n",
        "        target_mask = target[\"attention_mask\"].squeeze()\n",
        "\n",
        "        return {\n",
        "            \"source_ids\": source_ids.to(dtype=torch.long),\n",
        "            \"source_mask\": source_mask.to(dtype=torch.long),\n",
        "            \"target_ids\": target_ids.to(dtype=torch.long),\n",
        "            \"target_ids_y\": target_ids.to(dtype=torch.long),\n",
        "        }\n",
        "\n",
        "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
        "\n",
        "    \"\"\"\n",
        "    Function to be called for training with the parameters passed from main function\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    model.train()\n",
        "    train_losses = 0.0\n",
        "    for _, data in enumerate(loader, 0):\n",
        "        y = data[\"target_ids\"].to(device, dtype=torch.long)\n",
        "        y_ids = y[:, :-1].contiguous()\n",
        "        lm_labels = y[:, 1:].clone().detach()\n",
        "        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
        "        ids = data[\"source_ids\"].to(device, dtype=torch.long)\n",
        "        mask = data[\"source_mask\"].to(device, dtype=torch.long)\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=ids,\n",
        "            attention_mask=mask,\n",
        "            decoder_input_ids=y_ids,\n",
        "            labels=lm_labels,\n",
        "        )\n",
        "        loss = outputs[0]\n",
        "        train_losses += loss\n",
        "\n",
        "        if _ % 10 == 0:\n",
        "            training_logger.add_row(str(epoch), str(_), str(loss))\n",
        "            console.print(training_logger)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    average_train_loss = train_losses / len(loader)\n",
        "    return average_train_loss\n",
        "\n",
        "def validate_on_dev(epoch, tokenizer, model, device, loader, optimizer):\n",
        "\n",
        "    \"\"\"\n",
        "    Function to be called for training with the parameters passed from main function\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "    dev_losses = 0.0\n",
        "    with torch.no_grad():\n",
        "        for _, data in enumerate(loader, 0):\n",
        "            y = data[\"target_ids\"].to(device, dtype=torch.long)\n",
        "            y_ids = y[:, :-1].contiguous()\n",
        "            lm_labels = y[:, 1:].clone().detach()\n",
        "            lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
        "            ids = data[\"source_ids\"].to(device, dtype=torch.long)\n",
        "            mask = data[\"source_mask\"].to(device, dtype=torch.long)\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=ids,\n",
        "                attention_mask=mask,\n",
        "                decoder_input_ids=y_ids,\n",
        "                labels=lm_labels,\n",
        "            )\n",
        "            loss = outputs[0]\n",
        "            dev_losses += loss\n",
        "\n",
        "            # if _ % 10 == 0:\n",
        "            #     training_logger.add_row(str(epoch), str(_), str(loss))\n",
        "            #     console.print(training_logger)\n",
        "\n",
        "    average_dev_loss = dev_losses / len(loader)\n",
        "    return average_dev_loss\n",
        "\n",
        "\n",
        "def validate(epoch, tokenizer, model, device, loader):\n",
        "\n",
        "  \"\"\"\n",
        "  Function to evaluate model for predictions\n",
        "\n",
        "  \"\"\"\n",
        "  model.eval()\n",
        "  predictions = []\n",
        "  actuals = []\n",
        "  with torch.no_grad():\n",
        "      for _, data in enumerate(loader, 0):\n",
        "          y = data['target_ids'].to(device, dtype = torch.long)\n",
        "          ids = data['source_ids'].to(device, dtype = torch.long)\n",
        "          mask = data['source_mask'].to(device, dtype = torch.long)\n",
        "\n",
        "          generated_ids = model.generate(\n",
        "              input_ids = ids,\n",
        "              attention_mask = mask, \n",
        "              max_length=150, \n",
        "              num_beams=2,\n",
        "              repetition_penalty=2.5, \n",
        "              length_penalty=1.0, \n",
        "              early_stopping=True\n",
        "              )\n",
        "          preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in generated_ids]\n",
        "          target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=False)for t in y]\n",
        "          if _%10==0:\n",
        "              console.print(f'Completed {_}')\n",
        "\n",
        "          predictions.extend(preds)\n",
        "          actuals.extend(target)\n",
        "  return predictions, actuals   \n",
        "\n",
        "def BARTTrainer(\n",
        "    dataframe, source_text, target_text, model_params, output_dir=\"./outputs/\"\n",
        "):\n",
        "\n",
        "    \"\"\"\n",
        "    BART trainer\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Set random seeds and deterministic pytorch for reproducibility\n",
        "    torch.manual_seed(model_params[\"SEED\"])  # pytorch random seed\n",
        "    np.random.seed(model_params[\"SEED\"])  # numpy random seed\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "    # logging\n",
        "    console.log(f\"\"\"[Model]: Loading {model_params[\"MODEL\"]}...\\n\"\"\")\n",
        "\n",
        "    # tokenzier for encoding the text\n",
        "    tokenizer = BartTokenizer.from_pretrained(model_params[\"MODEL\"],dropout=0.5)\n",
        "\n",
        "    # Defining the model. We are using bart-base model and added a Language model layer on top for generation of Summary.\n",
        "    # Further this model is sent to device (GPU/TPU) for using the hardware.\n",
        "    model = BartForConditionalGeneration.from_pretrained(model_params[\"MODEL\"],dropout=0.5)\n",
        "    model = model.to(device)\n",
        "\n",
        "    # logging\n",
        "    console.log(f\"[Data]: Reading data...\\n\")\n",
        "\n",
        "    # Importing the raw dataset\n",
        "    dataframe = dataframe[[source_text, target_text]]\n",
        "    display_df(dataframe.head(2))\n",
        "\n",
        "    # Creation of Dataset and Dataloader\n",
        "    # Defining the train size. So 80% of the data will be used for training and the rest for validation.\n",
        "    #train_size = 0.8\n",
        "    #train_dataset = dataframe.sample(frac=train_size, random_state=model_params[\"SEED\"])\n",
        "    #val_dataset = dataframe.drop(train_dataset.index).reset_index(drop=True)\n",
        "    #train_dataset = train_dataset.reset_index(drop=True)\n",
        "    train_dataset = dataframe\n",
        "\n",
        "    console.print(f\"FULL Dataset: {dataframe.shape}\")\n",
        "    console.print(f\"TRAIN Dataset: {train_dataset.shape}\")\n",
        "    console.print(f\"Dev Dataset: {dev_dataset.shape}\")\n",
        "    console.print(f\"TEST Dataset: {val_dataset.shape}\\n\")\n",
        "\n",
        "    # Creating the Training and Validation dataset for further creation of Dataloader\n",
        "    training_set = YourDataSetClass(\n",
        "        train_dataset,\n",
        "        tokenizer,\n",
        "        model_params[\"MAX_SOURCE_TEXT_LENGTH\"],\n",
        "        model_params[\"MAX_TARGET_TEXT_LENGTH\"],\n",
        "        source_text,\n",
        "        target_text,\n",
        "    )\n",
        "    val_set = YourDataSetClass(\n",
        "        val_dataset,\n",
        "        tokenizer,\n",
        "        model_params[\"MAX_SOURCE_TEXT_LENGTH\"],\n",
        "        model_params[\"MAX_TARGET_TEXT_LENGTH\"],\n",
        "        source_text,\n",
        "        target_text,\n",
        "    )\n",
        "\n",
        "    dev_set = YourDataSetClass(\n",
        "        dev_dataset,\n",
        "        tokenizer,\n",
        "        model_params[\"MAX_SOURCE_TEXT_LENGTH\"],\n",
        "        model_params[\"MAX_TARGET_TEXT_LENGTH\"],\n",
        "        source_text,\n",
        "        target_text,\n",
        "    )\n",
        "\n",
        "    # Defining the parameters for creation of dataloaders\n",
        "    train_params = {\n",
        "        \"batch_size\": model_params[\"TRAIN_BATCH_SIZE\"],\n",
        "        \"shuffle\": True,\n",
        "        \"num_workers\": 0,\n",
        "    }\n",
        "\n",
        "    val_params = {\n",
        "        \"batch_size\": model_params[\"VALID_BATCH_SIZE\"],\n",
        "        \"shuffle\": False,\n",
        "        \"num_workers\": 0,\n",
        "    }\n",
        "    dev_params = {\n",
        "        \"batch_size\": model_params[\"VALID_BATCH_SIZE\"],\n",
        "        \"shuffle\": False,\n",
        "        \"num_workers\": 0,\n",
        "    }\n",
        "\n",
        "    # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n",
        "    training_loader = DataLoader(training_set, **train_params)\n",
        "    val_loader = DataLoader(val_set, **val_params)\n",
        "    dev_loader = DataLoader(dev_set, **dev_params)\n",
        "\n",
        "    # Defining the optimizer that will be used to tune the weights of the network in the training session.\n",
        "    optimizer = torch.optim.Adam(\n",
        "        params=model.parameters(), lr=model_params[\"LEARNING_RATE\"]\n",
        "    )\n",
        "\n",
        "    # Training loop\n",
        "    console.log(f\"[Initiating Fine Tuning]...\\n\")\n",
        "\n",
        "    for epoch in range(model_params[\"TRAIN_EPOCHS\"]):\n",
        "        train_loss = train(epoch, tokenizer, model, device, training_loader, optimizer)\n",
        "        # TODO: add dev loss evaluate + report\n",
        "        dev_loss = validate_on_dev(epoch, tokenizer, model, device, dev_loader, optimizer)\n",
        "        console.log(f\"epoch {epoch}, training loss: {train_loss}, dev loss: {dev_loss}\")\n",
        "\n",
        "    console.log(f\"[Saving Model]...\\n\")\n",
        "    # Saving the model after training\n",
        "    path = os.path.join(output_dir, \"model_files\")\n",
        "    model.save_pretrained(path)\n",
        "    tokenizer.save_pretrained(path)\n",
        "\n",
        "    # evaluating test dataset\n",
        "    console.log(f\"[Initiating Validation]...\\n\")\n",
        "    for epoch in range(model_params[\"VAL_EPOCHS\"]):\n",
        "        predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n",
        "        final_df = pd.DataFrame({\"Generated Text\": predictions, \"Actual Text\": actuals})\n",
        "        final_df.to_csv(os.path.join(output_dir, \"predictions.csv\"))\n",
        "\n",
        "    console.save_text(os.path.join(output_dir, \"logs.txt\"))\n",
        "\n",
        "    console.log(f\"[Validation Completed.]\\n\")\n",
        "    console.print(\n",
        "        f\"\"\"[Model] Model saved @ {os.path.join(output_dir, \"model_files\")}\\n\"\"\"\n",
        "    )\n",
        "    console.print(\n",
        "        f\"\"\"[Validation] Generation on Validation data saved @ {os.path.join(output_dir,'predictions.csv')}\\n\"\"\"\n",
        "    )\n",
        "    console.print(f\"\"\"[Logs] Logs saved @ {os.path.join(output_dir,'logs.txt')}\\n\"\"\")       \n",
        "\n",
        "\n",
        "# 1) Change model to facebook/bart-base\n",
        "# 2) Train epochs: 7/8\n",
        "# 3) Split the data (documents) every X sentences\n",
        "# Goal: generated.csv, outputs/predictions.csv\n",
        "\n",
        "model_params = {\n",
        "    \"MODEL\": \"facebook/bart-base\",  # model_type:\n",
        "    \"TRAIN_BATCH_SIZE\": 1,  # training batch size\n",
        "    \"VALID_BATCH_SIZE\": 1,  # validation batch size\n",
        "    \"TRAIN_EPOCHS\": 6,  # number of training epochs\n",
        "    \"VAL_EPOCHS\": 1,  # number of validation epochs\n",
        "    \"LEARNING_RATE\": 1e-4,  # learning rate\n",
        "    \"MAX_SOURCE_TEXT_LENGTH\": 512,  # max length of source text\n",
        "    \"MAX_TARGET_TEXT_LENGTH\": 512,  # max length of target text\n",
        "    \"SEED\": 42,  # set seed for reproducibility\n",
        "}\n",
        "\n",
        "train_path = '/content/output/train_Bart_format.csv'\n",
        "dev_path = '/content/output/dev_Bart_format.csv'\n",
        "test_path = '/content/output/test_Bart_format.csv'\n",
        "\n",
        "df = pd.read_csv(train_path)\n",
        "dev_dataset = pd.read_csv(dev_path)\n",
        "val_dataset = pd.read_csv(test_path)\n",
        "\n",
        "# TODO: check that data is read correctly (with commas in the input)\n",
        "\n",
        "# T5 accepts prefix of the task to be performed:\n",
        "\n",
        "BARTTrainer(\n",
        "    dataframe=df,\n",
        "    source_text=\"input\",\n",
        "    target_text=\"output\",\n",
        "    model_params=model_params,\n",
        "    output_dir=\"/content/outputsBART/\",\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cGiVWRjCMNe"
      },
      "source": [
        "### **Bart No Pretraining - model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtGjfzWykj0J"
      },
      "source": [
        "# install libraries\n",
        "!pip install sentencepiece\n",
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install rich[jupyter]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7l-9pOaoCL9T"
      },
      "source": [
        "# Importing libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import os\n",
        "\n",
        "# Importing the BART modules from huggingface/transformers\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration, BartConfig\n",
        "\n",
        "\n",
        "# rich: for a better display on terminal\n",
        "from rich.table import Column, Table\n",
        "from rich import box\n",
        "from rich.console import Console\n",
        "\n",
        "# define a rich console logger\n",
        "console = Console(record=True)\n",
        "\n",
        "# to display dataframe in ASCII format\n",
        "def display_df(df):\n",
        "    \"\"\"display dataframe in ASCII format\"\"\"\n",
        "\n",
        "    console = Console()\n",
        "    table = Table(\n",
        "        Column(\"source_text\", justify=\"center\"),\n",
        "        Column(\"target_text\", justify=\"center\"),\n",
        "        title=\"Sample Data\",\n",
        "        pad_edge=False,\n",
        "        box=box.ASCII,\n",
        "    )\n",
        "\n",
        "    for i, row in enumerate(df.values.tolist()):\n",
        "        table.add_row(row[0], row[1])\n",
        "\n",
        "    console.print(table)\n",
        "\n",
        "# training logger to log training progress\n",
        "training_logger = Table(\n",
        "    Column(\"Epoch\", justify=\"center\"),\n",
        "    Column(\"Steps\", justify=\"center\"),\n",
        "    Column(\"Loss\", justify=\"center\"),\n",
        "    title=\"Training Status\",\n",
        "    pad_edge=False,\n",
        "    box=box.ASCII,\n",
        ")\n",
        "\n",
        "# Setting up the device for GPU usage\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "\n",
        "class YourDataSetClass(Dataset):\n",
        "    \"\"\"\n",
        "    Creating a custom dataset for reading the dataset and\n",
        "    loading it into the dataloader to pass it to the\n",
        "    neural network for finetuning the model\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, dataframe, tokenizer, source_len, target_len, source_text, target_text\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initializes a Dataset class\n",
        "\n",
        "        Args:\n",
        "            dataframe (pandas.DataFrame): Input dataframe\n",
        "            tokenizer (transformers.tokenizer): Transformers tokenizer\n",
        "            source_len (int): Max length of source text\n",
        "            target_len (int): Max length of target text\n",
        "            source_text (str): column name of source text\n",
        "            target_text (str): column name of target text\n",
        "        \"\"\"\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.source_len = source_len\n",
        "        self.summ_len = target_len\n",
        "        self.target_text = self.data[target_text]\n",
        "        self.source_text = self.data[source_text]\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"returns the length of dataframe\"\"\"\n",
        "\n",
        "        return len(self.target_text)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"return the input ids, attention masks and target ids\"\"\"\n",
        "\n",
        "        source_text = str(self.source_text[index])\n",
        "        target_text = str(self.target_text[index])\n",
        "\n",
        "        # cleaning data so as to ensure data is in string type\n",
        "        source_text = \" \".join(source_text.split())\n",
        "        target_text = \" \".join(target_text.split())\n",
        "\n",
        "        source = self.tokenizer.batch_encode_plus(\n",
        "            [source_text],\n",
        "            max_length=self.source_len,\n",
        "            pad_to_max_length=True,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        target = self.tokenizer.batch_encode_plus(\n",
        "            [target_text],\n",
        "            max_length=self.summ_len,\n",
        "            pad_to_max_length=True,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        source_ids = source[\"input_ids\"].squeeze()\n",
        "        source_mask = source[\"attention_mask\"].squeeze()\n",
        "        target_ids = target[\"input_ids\"].squeeze()\n",
        "        target_mask = target[\"attention_mask\"].squeeze()\n",
        "\n",
        "        return {\n",
        "            \"source_ids\": source_ids.to(dtype=torch.long),\n",
        "            \"source_mask\": source_mask.to(dtype=torch.long),\n",
        "            \"target_ids\": target_ids.to(dtype=torch.long),\n",
        "            \"target_ids_y\": target_ids.to(dtype=torch.long),\n",
        "        }\n",
        "\n",
        "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
        "\n",
        "    \"\"\"\n",
        "    Function to be called for training with the parameters passed from main function\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    model.train()\n",
        "    train_losses = 0.0\n",
        "    for _, data in enumerate(loader, 0):\n",
        "        y = data[\"target_ids\"].to(device, dtype=torch.long)\n",
        "        y_ids = y[:, :-1].contiguous()\n",
        "        lm_labels = y[:, 1:].clone().detach()\n",
        "        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
        "        ids = data[\"source_ids\"].to(device, dtype=torch.long)\n",
        "        mask = data[\"source_mask\"].to(device, dtype=torch.long)\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=ids,\n",
        "            attention_mask=mask,\n",
        "            decoder_input_ids=y_ids,\n",
        "            labels=lm_labels,\n",
        "        )\n",
        "        loss = outputs[0]\n",
        "        train_losses += loss\n",
        "\n",
        "        if _ % 10 == 0:\n",
        "            training_logger.add_row(str(epoch), str(_), str(loss))\n",
        "            console.print(training_logger)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    average_train_loss = train_losses / len(loader)\n",
        "    return average_train_loss\n",
        "\n",
        "def validate_on_dev(epoch, tokenizer, model, device, loader, optimizer):\n",
        "\n",
        "    \"\"\"\n",
        "    Function to be called for training with the parameters passed from main function\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "    dev_losses = 0.0\n",
        "    with torch.no_grad():\n",
        "        for _, data in enumerate(loader, 0):\n",
        "            y = data[\"target_ids\"].to(device, dtype=torch.long)\n",
        "            y_ids = y[:, :-1].contiguous()\n",
        "            lm_labels = y[:, 1:].clone().detach()\n",
        "            lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
        "            ids = data[\"source_ids\"].to(device, dtype=torch.long)\n",
        "            mask = data[\"source_mask\"].to(device, dtype=torch.long)\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=ids,\n",
        "                attention_mask=mask,\n",
        "                decoder_input_ids=y_ids,\n",
        "                labels=lm_labels,\n",
        "            )\n",
        "            loss = outputs[0]\n",
        "            dev_losses += loss\n",
        "\n",
        "            if _ % 10 == 0:\n",
        "                training_logger.add_row(str(epoch), str(_), str(loss))\n",
        "                console.print(training_logger)\n",
        "\n",
        "    average_dev_loss = dev_losses / len(loader)\n",
        "    return average_dev_loss\n",
        "\n",
        "\n",
        "def validate(epoch, tokenizer, model, device, loader):\n",
        "\n",
        "  \"\"\"\n",
        "  Function to evaluate model for predictions\n",
        "\n",
        "  \"\"\"\n",
        "  model.eval()\n",
        "  predictions = []\n",
        "  actuals = []\n",
        "  with torch.no_grad():\n",
        "      for _, data in enumerate(loader, 0):\n",
        "          y = data['target_ids'].to(device, dtype = torch.long)\n",
        "          ids = data['source_ids'].to(device, dtype = torch.long)\n",
        "          mask = data['source_mask'].to(device, dtype = torch.long)\n",
        "\n",
        "          generated_ids = model.generate(\n",
        "              input_ids = ids,\n",
        "              attention_mask = mask, \n",
        "              max_length=150, \n",
        "              num_beams=2,\n",
        "              repetition_penalty=2.5, \n",
        "              length_penalty=1.0, \n",
        "              early_stopping=True\n",
        "              )\n",
        "          preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
        "          target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
        "          if _%10==0:\n",
        "              console.print(f'Completed {_}')\n",
        "\n",
        "          predictions.extend(preds)\n",
        "          actuals.extend(target)\n",
        "  return predictions, actuals   \n",
        "\n",
        "def BARTTrainer(\n",
        "    dataframe, source_text, target_text, model_params, output_dir=\"./outputs/\"\n",
        "):\n",
        "\n",
        "    \"\"\"\n",
        "    BART trainer\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Set random seeds and deterministic pytorch for reproducibility\n",
        "    torch.manual_seed(model_params[\"SEED\"])  # pytorch random seed\n",
        "    np.random.seed(model_params[\"SEED\"])  # numpy random seed\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "    # logging\n",
        "    console.log(f\"\"\"[Model]: Loading {model_params[\"MODEL\"]}...\\n\"\"\")\n",
        "\n",
        "    # tokenzier for encoding the text\n",
        "    tokenizer = BartTokenizer.from_pretrained(model_params[\"MODEL\"])\n",
        "\n",
        "    # Defining the model. We are using bart-base model and added a Language model layer on top for generation of Summary.\n",
        "    # Further this model is sent to device (GPU/TPU) for using the hardware.\n",
        "    config = BartConfig(dropout=0.1)\n",
        "    model = BartForConditionalGeneration(config)\n",
        "    model = model.to(device)\n",
        "\n",
        "    # logging\n",
        "    console.log(f\"[Data]: Reading data...\\n\")\n",
        "\n",
        "    # Importing the raw dataset\n",
        "    dataframe = dataframe[[source_text, target_text]]\n",
        "    display_df(dataframe.head(2))\n",
        "\n",
        "    # Creation of Dataset and Dataloader\n",
        "    # Defining the train size. So 80% of the data will be used for training and the rest for validation.\n",
        "    #train_size = 0.8\n",
        "    #train_dataset = dataframe.sample(frac=train_size, random_state=model_params[\"SEED\"])\n",
        "    #val_dataset = dataframe.drop(train_dataset.index).reset_index(drop=True)\n",
        "    #train_dataset = train_dataset.reset_index(drop=True)\n",
        "    train_dataset = dataframe\n",
        "\n",
        "    console.print(f\"FULL Dataset: {dataframe.shape}\")\n",
        "    console.print(f\"TRAIN Dataset: {train_dataset.shape}\")\n",
        "    console.print(f\"Dev Dataset: {dev_dataset.shape}\")\n",
        "    console.print(f\"TEST Dataset: {val_dataset.shape}\\n\")\n",
        "\n",
        "    # Creating the Training and Validation dataset for further creation of Dataloader\n",
        "    training_set = YourDataSetClass(\n",
        "        train_dataset,\n",
        "        tokenizer,\n",
        "        model_params[\"MAX_SOURCE_TEXT_LENGTH\"],\n",
        "        model_params[\"MAX_TARGET_TEXT_LENGTH\"],\n",
        "        source_text,\n",
        "        target_text,\n",
        "    )\n",
        "    val_set = YourDataSetClass(\n",
        "        val_dataset,\n",
        "        tokenizer,\n",
        "        model_params[\"MAX_SOURCE_TEXT_LENGTH\"],\n",
        "        model_params[\"MAX_TARGET_TEXT_LENGTH\"],\n",
        "        source_text,\n",
        "        target_text,\n",
        "    )\n",
        "\n",
        "    dev_set = YourDataSetClass(\n",
        "        dev_dataset,\n",
        "        tokenizer,\n",
        "        model_params[\"MAX_SOURCE_TEXT_LENGTH\"],\n",
        "        model_params[\"MAX_TARGET_TEXT_LENGTH\"],\n",
        "        source_text,\n",
        "        target_text,\n",
        "    )\n",
        "\n",
        "    # Defining the parameters for creation of dataloaders\n",
        "    train_params = {\n",
        "        \"batch_size\": model_params[\"TRAIN_BATCH_SIZE\"],\n",
        "        \"shuffle\": True,\n",
        "        \"num_workers\": 0,\n",
        "    }\n",
        "\n",
        "    val_params = {\n",
        "        \"batch_size\": model_params[\"VALID_BATCH_SIZE\"],\n",
        "        \"shuffle\": False,\n",
        "        \"num_workers\": 0,\n",
        "    }\n",
        "    dev_params = {\n",
        "        \"batch_size\": model_params[\"VALID_BATCH_SIZE\"],\n",
        "        \"shuffle\": False,\n",
        "        \"num_workers\": 0,\n",
        "    }\n",
        "\n",
        "    # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n",
        "    training_loader = DataLoader(training_set, **train_params)\n",
        "    val_loader = DataLoader(val_set, **val_params)\n",
        "    dev_loader = DataLoader(dev_set, **dev_params)\n",
        "\n",
        "    # Defining the optimizer that will be used to tune the weights of the network in the training session.\n",
        "    optimizer = torch.optim.Adam(\n",
        "        params=model.parameters(), lr=model_params[\"LEARNING_RATE\"]\n",
        "    )\n",
        "\n",
        "    # Training loop\n",
        "    console.log(f\"[Initiating Fine Tuning]...\\n\")\n",
        "\n",
        "    for epoch in range(model_params[\"TRAIN_EPOCHS\"]):\n",
        "        train_loss = train(epoch, tokenizer, model, device, training_loader, optimizer)\n",
        "        # TODO: add dev loss evaluate + report\n",
        "        dev_loss = validate_on_dev(epoch, tokenizer, model, device, dev_loader, optimizer)\n",
        "        console.log(f\"epoch {epoch}, training loss: {train_loss}, dev loss: {dev_loss}\")\n",
        "\n",
        "    console.log(f\"[Saving Model]...\\n\")\n",
        "    # Saving the model after training\n",
        "    path = os.path.join(output_dir, \"model_files\")\n",
        "    model.save_pretrained(path)\n",
        "    tokenizer.save_pretrained(path)\n",
        "\n",
        "    # evaluating test dataset\n",
        "    console.log(f\"[Initiating Validation]...\\n\")\n",
        "    for epoch in range(model_params[\"VAL_EPOCHS\"]):\n",
        "        predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n",
        "        final_df = pd.DataFrame({\"Generated Text\": predictions, \"Actual Text\": actuals})\n",
        "        final_df.to_csv(os.path.join(output_dir, \"predictions.csv\"))\n",
        "\n",
        "    console.save_text(os.path.join(output_dir, \"logs.txt\"))\n",
        "\n",
        "    console.log(f\"[Validation Completed.]\\n\")\n",
        "    console.print(\n",
        "        f\"\"\"[Model] Model saved @ {os.path.join(output_dir, \"model_files\")}\\n\"\"\"\n",
        "    )\n",
        "    console.print(\n",
        "        f\"\"\"[Validation] Generation on Validation data saved @ {os.path.join(output_dir,'predictions.csv')}\\n\"\"\"\n",
        "    )\n",
        "    console.print(f\"\"\"[Logs] Logs saved @ {os.path.join(output_dir,'logs.txt')}\\n\"\"\")       \n",
        "\n",
        "\n",
        "# 1) Change model to facebook/bart-base\n",
        "# 2) Train epochs: 7/8\n",
        "# 3) Split the data (documents) every X sentences\n",
        "# Goal: generated.csv, outputs/predictions.csv\n",
        "\n",
        "model_params = {\n",
        "    \"MODEL\": \"facebook/bart-base\",  # model_type: t5-base/t5-large\n",
        "    \"TRAIN_BATCH_SIZE\": 1,  # training batch size\n",
        "    \"VALID_BATCH_SIZE\": 1,  # validation batch size\n",
        "    \"TRAIN_EPOCHS\": 3,  # number of training epochs\n",
        "    \"VAL_EPOCHS\": 1,  # number of validation epochs\n",
        "    \"LEARNING_RATE\": 1e-4,  # learning rate\n",
        "    \"MAX_SOURCE_TEXT_LENGTH\": 512,  # max length of source text\n",
        "    \"MAX_TARGET_TEXT_LENGTH\": 512,  # max length of target text\n",
        "    \"SEED\": 42,  # set seed for reproducibility\n",
        "}\n",
        "\n",
        "train_path = '/content/output/train_Bart_format.csv'\n",
        "dev_path = '/content/output/dev_Bart_format.csv'\n",
        "test_path = '/content/output/test_Bart_format.csv'\n",
        "\n",
        "df = pd.read_csv(train_path)\n",
        "dev_dataset = pd.read_csv(dev_path)\n",
        "val_dataset = pd.read_csv(test_path)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# TODO: check that data is read correctly (with commas in the input)\n",
        "\n",
        "# T5 accepts prefix of the task to be performed:\n",
        "\n",
        "BARTTrainer(\n",
        "    dataframe=df,\n",
        "    source_text=\"input\",\n",
        "    target_text=\"output\",\n",
        "    model_params=model_params,\n",
        "    output_dir=\"/content/outputsBartNoPre/\",\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ru3SDltVJAoJ"
      },
      "source": [
        "### **From predictions to jsonlines**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnpOhd3sJHaD"
      },
      "source": [
        "!pip install jsonlines\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWHokWwRJAOs"
      },
      "source": [
        "import csv\n",
        "import json\n",
        "import jsonlines\n",
        "\n",
        "\n",
        "def convert(inputpath):\n",
        "  with jsonlines.open('generate.JSONLINES', mode='w') as generateW:\n",
        "      with jsonlines.open('actual.JSONLINES', mode='w') as actualW:\n",
        "        with open(inputpath) as f:\n",
        "            reader = csv.reader(f)\n",
        "            count = 0 \n",
        "\n",
        "            for row in reader:\n",
        "              \n",
        "              start_index = 0\n",
        "              end_index = 0\n",
        "\n",
        "              actual = row[2]\n",
        "              actuals=\"\"\n",
        "              max = -1\n",
        "              for i in range(len(actual)):\n",
        "                if actual[i] != \" \":\n",
        "                  actuals+=actual[i]\n",
        "                if (actual[i]).isdigit():\n",
        "                  if max < int(actual[i]):\n",
        "                    max = int(actual[i])\n",
        "              #print(actuals)\n",
        "              actual_clusters = []\n",
        "              if max != -1:\n",
        "                for i in range(max + 1):\n",
        "                  actual_clusters.append([])\n",
        "                actual_index = 0\n",
        "                while actual_index < len(actuals):\n",
        "                  if actuals[actual_index].isdigit():\n",
        "                    start_index = actual_index\n",
        "                    end_index = actual_index\n",
        "                    while (end_index+1) < len(actuals):\n",
        "                      if actuals[start_index] == actuals[end_index+1]:\n",
        "                        end_index += 1\n",
        "                      else:\n",
        "                        break\n",
        "                    actual_clusters[int(actuals[actual_index])].append([start_index, end_index])\n",
        "                    actual_index = end_index + 1\n",
        "                  else:\n",
        "                    actual_index += 1\n",
        "            \n",
        "              #print(actual_clusters)\n",
        "              actualW.write(tuple(actual_clusters))\n",
        "\n",
        "              generate = row[1]\n",
        "              generates=\"\"\n",
        "              max = -1\n",
        "              for i in range(len(generate)):\n",
        "                if generate[i] != \" \":\n",
        "                  generates+=generate[i]\n",
        "                if (generate[i]).isdigit():\n",
        "                  if max < int(generate[i]):\n",
        "                    max = int(generate[i])\n",
        "              #print(generates)\n",
        "        \n",
        "              generate_clusters = []\n",
        "              count = 0\n",
        "              if max != -1:\n",
        "                for i in range(max + 1):\n",
        "                  generate_clusters.append([])\n",
        "                generate_index = 0\n",
        "                while generate_index < len(generates):\n",
        "                  if generates[generate_index].isdigit():\n",
        "                    start_index = generate_index\n",
        "                    end_index = generate_index\n",
        "                    while (end_index+1) < len(generates):\n",
        "                      if generates[start_index] == generates[end_index+1]:\n",
        "                        end_index += 1\n",
        "                      else:\n",
        "                        break\n",
        "                    generate_clusters[int(generates[generate_index])].append(tuple([start_index, end_index]))\n",
        "                    generate_index = end_index + 1\n",
        "                  else:\n",
        "                    generate_index += 1\n",
        "              #print(generate_clusters)\n",
        "              generate_clusters = tuple(generate_clusters)\n",
        "              generateW.write(generate_clusters)\n",
        "\n",
        "def main():\n",
        "  inputpath = '/content/predictions.csv'\n",
        "  convert(inputpath)\n",
        "  # convert()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuqmikuRKSGX"
      },
      "source": [
        "### **MUC & B3 & CEAF** - **F1 , Recall , Precision**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlL0lXAbCvcI"
      },
      "source": [
        "!pip install jsonlines\n",
        "!pip install rich[jupyter]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFp5oFlY-qeP"
      },
      "source": [
        "import jsonlines\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from rich.table import Column, Table\n",
        "from rich.console import Console\n",
        "from rich import box\n",
        "\n",
        "\n",
        "def extract_mentions_to_predicted_clusters_from_clusters(num_cluster,actual_clusters):\n",
        "    mention_to_gold = {}\n",
        "    for gc in range(num_cluster):\n",
        "        for mention in actual_clusters[gc]:\n",
        "            mention_to_gold[tuple(mention)] = str(gc)\n",
        "    return mention_to_gold\n",
        "\n",
        "def phi4(c1, c2):\n",
        "    return 0 if len(c1) + len(c2) == 0 else 2 * len([m for m in c1 if m in c2]) / float(len(c1) + len(c2))\n",
        "\n",
        "def ceafe(clusters, gold_clusters):\n",
        "    clusters = [c for c in clusters if len(c) != 1]\n",
        "    scores = np.zeros((len(gold_clusters), len(clusters)))\n",
        "    for i in range(len(gold_clusters)):\n",
        "        for j in range(len(clusters)):\n",
        "            scores[i, j] = phi4(gold_clusters[i], clusters[j])\n",
        "    row_ind, col_ind = linear_sum_assignment(-scores)\n",
        "    similarity = sum(scores[row_ind, col_ind])\n",
        "    return similarity, len(clusters), similarity, len(gold_clusters)\n",
        "\n",
        "\n",
        "def b_cubed(clusters, mention_to_gold):\n",
        "  num, dem = 0, 0\n",
        "  for c in clusters:\n",
        "    if len(c) == 1:\n",
        "      continue\n",
        "    gold_counts = Counter()\n",
        "    correct = 0\n",
        "    for m in c:\n",
        "      m = tuple(m)\n",
        "      if m in mention_to_gold:\n",
        "        gold_counts[tuple(mention_to_gold[m])] += 1\n",
        "        for c2, count in gold_counts.items():\n",
        "          if len(c2) != 1:\n",
        "            correct += count * count\n",
        "        num += correct / float(len(c))\n",
        "        dem += len(c)\n",
        "  return num, dem\n",
        "\n",
        "def muc(clusters, mention_to_gold):\n",
        "    tp, p = 0, 0\n",
        "    for c in clusters:\n",
        "      p += len(c) - 1\n",
        "      tp += len(c)\n",
        "      linked = set()\n",
        "      for m in c:\n",
        "        m = tuple(m)\n",
        "        if m in mention_to_gold:\n",
        "            linked.add(mention_to_gold[m])\n",
        "        else:\n",
        "            tp -= 1\n",
        "      tp -= len(linked)\n",
        "    return tp, p\n",
        "\n",
        "\n",
        "def get_f1(p_num , p_den , r_num, r_den,beta=1 ):\n",
        "  p = 0 if p_den == 0 else p_num / float(p_den)\n",
        "  r = 0 if r_den == 0 else r_num / float(r_den)\n",
        "  return 0 if p + r == 0 else (1 + beta * beta) * p * r / (beta * beta * p + r)\n",
        "\n",
        "def get_recall(p_num , p_den , r_num, r_den):\n",
        "  return 0 if r_num == 0 else r_num / float(r_den)\n",
        "\n",
        "def get_precision(p_num , p_den , r_num, r_den):\n",
        "  return 0 if p_num == 0 else p_num / float(p_den)\n",
        "\n",
        "\n",
        "\n",
        "def calc_4_values(type, actuals, predictions):\n",
        "\n",
        "  p_num = 0\n",
        "  p_den = 0\n",
        "  r_num = 0\n",
        "  r_den = 0\n",
        "\n",
        "  for i in range(len(actuals)):\n",
        "    line_actual = actuals[i]\n",
        "    line_predicted = predictions[i]\n",
        "    # if len(line_actual) > 0:\n",
        "    num_cluster = len(line_actual)\n",
        "    mention_to_gold = extract_mentions_to_predicted_clusters_from_clusters(num_cluster,line_actual)\n",
        "    # print(mention_to_gold)\n",
        "    num_cluster = len(line_predicted)\n",
        "    mention_to_predicted = extract_mentions_to_predicted_clusters_from_clusters(num_cluster,line_predicted)\n",
        "    # print(mention_to_predicted)\n",
        "\n",
        "    if type == 'ceafe':\n",
        "      pn , pd , rn, rd =  ceafe(line_predicted, line_actual)\n",
        "    elif type == 'muc':\n",
        "      pn , pd =  muc(line_predicted, mention_to_gold)\n",
        "      rn , rd =  muc(line_actual, mention_to_predicted)\n",
        "    else:\n",
        "      pn , pd =  b_cubed(line_predicted, mention_to_gold)\n",
        "      rn , rd =  b_cubed(line_actual, mention_to_predicted)\n",
        "    \n",
        "    p_num += pn\n",
        "    p_den += pd\n",
        "    r_num += rn\n",
        "    r_den += rd\n",
        "\n",
        "  return p_num , p_den, r_num, r_den\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "  Title = 'Results for model T5 with the parameters:' + '\\n' + ' - learning rate = 1e-5' + '\\n' + ' - Batch size = 1' + '\\n' + ' - Number of Epochs = 6' + '\\n' + ' - Number of Beam search = 2' \n",
        "  console = Console()\n",
        "  table_1 = Table(\n",
        "        Column(\"Model\", justify=\"center\"),\n",
        "        Column(\"Metric\", justify=\"center\"),\n",
        "        Column(\"F1\", justify=\"center\"),\n",
        "        Column(\"Recall\", justify=\"center\"),\n",
        "        Column(\"Precision\", justify=\"center\"),\n",
        "        title=Title,\n",
        "        pad_edge=False,\n",
        "        box=box.ASCII,\n",
        "  )\n",
        "\n",
        "  path_actual = '/content/actual.JSONLINES'\n",
        "  path_generate = '/content/generate.JSONLINES'\n",
        "  actuals = []\n",
        "  predictions = []\n",
        "  with jsonlines.open(path_actual) as reader:\n",
        "    for obj in reader:\n",
        "      actuals.append(obj)\n",
        "  with jsonlines.open(path_generate) as reader:\n",
        "    for obj in reader:\n",
        "      predictions.append(obj)\n",
        "\n",
        "  p_num_muc , p_den_muc, r_num_muc, r_den_muc  = calc_4_values('muc',actuals, predictions)\n",
        "\n",
        "  f1_muc = get_f1(p_num_muc , p_den_muc, r_num_muc, r_den_muc,beta=1)\n",
        "  recall_muc = get_recall(p_num_muc , p_den_muc, r_num_muc, r_den_muc)\n",
        "  precision_muc = get_precision(p_num_muc , p_den_muc, r_num_muc, r_den_muc)\n",
        "  #print (f1_muc , recall_muc , precision_muc)\n",
        "  table_1.add_row('Bart', 'MUC' ,str(f1_muc), str(recall_muc),str(precision_muc))\n",
        "\n",
        "\n",
        "  p_num_B3 , p_den_B3, r_num_B3, r_den_B3  = calc_4_values(None,actuals, predictions)\n",
        "\n",
        "  f1_B3 = get_f1(p_num_B3 , p_den_B3, r_num_B3, r_den_B3,beta=1)\n",
        "  recall_B3 = get_recall(p_num_B3 , p_den_B3, r_num_B3, r_den_B3)\n",
        "  precision_B3 = get_precision(p_num_B3 , p_den_B3, r_num_B3, r_den_B3)\n",
        "  #print (f1_B3 , recall_B3 , precision_B3)\n",
        "  table_1.add_row('Bart', 'B3' ,str(f1_B3), str(recall_B3),str(precision_B3))\n",
        "\n",
        "\n",
        "  p_num_CAEF , p_den_CAEF, r_num_CAEF, r_den_CAEF  = calc_4_values('ceafe',actuals, predictions)\n",
        "\n",
        "  f1_CAEF = get_f1(p_num_CAEF , p_den_CAEF, r_num_CAEF, r_den_CAEF,beta=1)\n",
        "  recall_CAEF = get_recall(p_num_CAEF , p_den_CAEF, r_num_CAEF, r_den_CAEF)\n",
        "  precision_CAEF = get_precision(p_num_CAEF , p_den_CAEF, r_num_CAEF, r_den_CAEF)\n",
        "  #print (f1_CAEF , recall_CAEF , precision_CAEF)\n",
        "  table_1.add_row('Bart', 'CAEF' ,str(f1_CAEF), str(recall_CAEF),str(precision_CAEF))\n",
        "\n",
        "  console.print(table_1)\n",
        "\n",
        "  table_2 = Table(\n",
        "        Column(\"Model\", justify=\"center\"),\n",
        "        Column(\"Avg_F1\", justify=\"center\"),\n",
        "        Column(\"Avg_Recall\", justify=\"center\"),\n",
        "        Column(\"Avg_Precision\", justify=\"center\"),\n",
        "        title=\"Averages\",\n",
        "        pad_edge=False,\n",
        "        box=box.ASCII,\n",
        "  )\n",
        "\n",
        "  F1_avg = (f1_muc + f1_B3 + f1_CAEF) / 3\n",
        "  Recall_avg = (recall_muc + recall_B3 + recall_CAEF) / 3\n",
        "  Precision_avg = (precision_muc + precision_B3 + precision_CAEF) / 3\n",
        "  table_2.add_row('Bart', str(F1_avg), str(Recall_avg),str(Precision_avg))\n",
        "  console.print(table_2)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}